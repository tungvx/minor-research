\label{sec:introduction}

Semantic parsing is a very important process in Natural Language Processing. It converts a natural language sentence into a special meaning representation so that computer programs can read and process it. An effective way to do this task is using machine learning to build a model of relationship between natural language structure and formal representation structure. Unfortunately, most of the current approaches require very large amount of fully annotated data in order to obtain a good model. Annotated data means that for each input sentence of natural language, the corresponding representation is also provided. State-of-the-art methodologies often use statistical analysis to build the model and thus generally ill-perform with unseen data. Therefore, in order to get good results, they need to collect a large amount of annotated corpus. The annotation, however, is almost prepared manually and thus is difficult and time consuming. \cite{Zelle:1996:LPD:1864519.1864543}, \cite{Tang:2001:UMC:645328.650015}, \cite{Zettlemoyer05learningto}, \cite{Ge:2005:SSP:1706543.1706546}, \cite{Zettlemoyer07onlinelearning} and \cite{Wong07learningsynchronous} are examples of works in this category.

Recently, \citeauthor{Clarke:2010:DSP:1870568.1870571} in \cite{Clarke:2010:DSP:1870568.1870571} implemented a new learning paradigm aimed at alleviating the supervision burden. The algorithm is able to predict complex structures which only rely on a binary feedback. Borrowing the idea from these authors, we developed a question answering system for Vietnamese with suitable feature computations.

\subsection*{Related Work}
\input{related}

The rest of this paper is organized as follow. Section \ref{sec:c-u} describes how natural language sentences are represented formally in our semantic parser. Section \ref{sec:model} illustrates the way we maintain the parsing model. We present our experiment result and discuss it in Section \ref{sec:experiment}. Finally, conclusion and future works are drawn in Section \ref{sec:conclusion}.