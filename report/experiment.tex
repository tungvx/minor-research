\label{sec:experiment}
We use the Geoquery domain for evaluating our system. It consists of a database and Prolog query language for U.S. geographical facts. The corpus has 880 English tokenized queries; each query is paired with a Prolog query. We use Google Translate API to translate the English queries into Vietnamese ones. The translation contains many errors as the result of some word-to-word translations. We then manually correct each query to produce meaningful Vietnamese queries. 

Following the experiment in \cite{Clarke:2010:DSP:1870568.1870571}, we randomly select 250 queries for training, 250 other queries for testing from the above 880 queries. Our experiment is used to answer the following doubt:

\begin{enumerate}
  \item What are the effects of dynamic compositional preferences learning?
  \item How do two learning approaches of \cite{Clarke:2010:DSP:1870568.1870571} perform in our system?
\end{enumerate}

\subsection{What are the effects of dynamic compositional preferences learning?}
In this experiment, we run our system with and without dynamic learning of compositional preferences. The experiment result is described in table \ref{d-c-p-eff}. 

"No dynamic learning" row means that we do not use dynamic compositional preferences learning. When calculating the features for function-function compositions, the second feature is the same as the first one. That means they are both computed based on the vector of distance between two tokens.

It is clear from the table that the strategy of dynamic preferences computation improves the accuracy of the system by $4.4\%$ and $3.2\%$ for training data and testing data respectively. We also may wonder that if some set of questions is asked many times by users, how does this strategy perform? The last row of table \ref{d-c-p-eff} shows that if we repeatedly run the training data 10 times, then the final result on the training data is surprisingly $62\%$. In addition, if we run on testing data after running 10 times on training data, the accuracy for testing data is not significantly affected. It decreases by 1.6\%. 
\begin{table}[h] 
	\begin{center}
	    \begin{tabular}{| p{5cm} | c | c |}
	    \hline
	    Algorithm & Training set & Testing set \\ \hline
		No dynamic learning & 53.6\% & 51.6\%  \\ \hline
	    Dynamic learning & 58\% & 54.8\%  \\ \hline
	    Dynamic learning with repetition on training data & 62\% & 53.2\%  \\
	    \hline
	    \end{tabular}        
	\end{center}
	\scriptsize
	\caption{Accuracy of model on traing data and testing data. "No dynamic learning" means that we do not learn compositional preferences during running time of system. "Dynamic learning" means that we apply this kind of learning. in "Dynamic learning with repetition on training data", we run the system 10 times on training data; then testing data is also experimented}
    \label{d-c-p-eff}
\end{table}

\subsection{How do two learning approaches of \cite{Clarke:2010:DSP:1870568.1870571} perform in our system?}
We implemented two learning mechanisms mentioned in \cite{Clarke:2010:DSP:1870568.1870571}. However, the "Direct Approach" or Binary classification did not successfully learn the weight vector. The learned vector contains negative values. This is because the way we choose the feature of "Token-Function mapping" is simple, just being word matching. We need some semantics features which measure the semantic similarity between words. \citeauthor{Clarke:2010:DSP:1870568.1870571} in \cite{Clarke:2010:DSP:1870568.1870571} used Wordnet for this purpose. Unfortunately, we do not have Wordnet for Vietnamese. But we had an idea of using word2vec, a tool of Google, to tackle this problem. This is left for our future works.

On the other hand, "Aggressive approach" can learn the weight vector but the result is not improved, around $51\%$. In future, we need to investigate more about this to take advantage from machine learning.